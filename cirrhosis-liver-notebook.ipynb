{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":60893,"databundleVersionId":7000181,"sourceType":"competition"},{"sourceId":6724823,"sourceType":"datasetVersion","datasetId":3873965}],"dockerImageVersionId":30626,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"**Purpose**\n\nThe purpose of this notebook is to give beginners an access point to learn about how to make an accessable model that someone new can begin to understand. Therefore, this notebook will be over commented to afford accessability. ","metadata":{}},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd \nfrom sklearn.preprocessing import OneHotEncoder, StandardScaler, LabelEncoder, OrdinalEncoder, RobustScaler\nimport seaborn as sn\nimport matplotlib.pyplot as plt\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.metrics import log_loss\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.impute import SimpleImputer, KNNImputer\nfrom sklearn.compose import make_column_selector as selector\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nimport xgboost as xgb\nfrom sklearn.mixture import GaussianMixture\nfrom sklearn.decomposition import PCA\n        \ntrain = pd.read_csv('/kaggle/input/playground-series-s3e26/train.csv') #import training dataset\ntrain = train.drop('id', axis = 1)\ntest = pd.read_csv('/kaggle/input/playground-series-s3e26/test.csv')\noriginal_data = pd.read_csv('/kaggle/input/cirrhosis-patient-survival-prediction/cirrhosis.csv')[train.columns] \ntrain = pd.concat(objs=[train, original_data]).reset_index(drop=True)\ntrain['Age'] = train['Age'] - train['N_Days']\n# train['Age'] = train['Age'] / 365\n# train = train.drop(['Drug', 'Sex'], axis = 1)\nindex = test['id']\ntest = test.drop('id', axis =1)\ntest['Age'] = test['Age'] - test['N_Days']\n# test = test.drop(['Drug', 'Sex'], axis = 1)\ntrain = train.dropna()\n# feature_names = list(train.columns[:-1])","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-01-01T19:51:29.739042Z","iopub.execute_input":"2024-01-01T19:51:29.739558Z","iopub.status.idle":"2024-01-01T19:51:29.828975Z","shell.execute_reply.started":"2024-01-01T19:51:29.739523Z","shell.execute_reply":"2024-01-01T19:51:29.827722Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names","metadata":{"execution":{"iopub.status.busy":"2024-01-01T18:28:48.286217Z","iopub.execute_input":"2024-01-01T18:28:48.286640Z","iopub.status.idle":"2024-01-01T18:28:48.294667Z","shell.execute_reply.started":"2024-01-01T18:28:48.286607Z","shell.execute_reply":"2024-01-01T18:28:48.293004Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T18:19:16.627337Z","iopub.execute_input":"2024-01-01T18:19:16.627804Z","iopub.status.idle":"2024-01-01T18:19:16.660405Z","shell.execute_reply.started":"2024-01-01T18:19:16.627768Z","shell.execute_reply":"2024-01-01T18:19:16.658850Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train.dropna()\ntrain_df = train\ntest_df = test","metadata":{"execution":{"iopub.status.busy":"2024-01-01T18:28:52.999993Z","iopub.execute_input":"2024-01-01T18:28:53.000536Z","iopub.status.idle":"2024-01-01T18:28:53.018838Z","shell.execute_reply.started":"2024-01-01T18:28:53.000499Z","shell.execute_reply":"2024-01-01T18:28:53.017468Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:41:32.447911Z","iopub.execute_input":"2024-01-01T17:41:32.448434Z","iopub.status.idle":"2024-01-01T17:41:32.481153Z","shell.execute_reply.started":"2024-01-01T17:41:32.448388Z","shell.execute_reply":"2024-01-01T17:41:32.479828Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def engineer_features(data):\n    col_drop = []\n    normal_ranges = {\n    'Bilirubin': (0.1, 1.2),  # Normal range of bilirubin in mg/dL\n    'Alk_Phos': (44, 147),    # Normal range of alkaline phosphatase in IU/L\n    'SGOT': (10, 40),         # Normal range of SGOT in U/L\n    'Albumin': (3.5, 5.0)     # Normal range of albumin in g/dL\n    }\n\n    for test, (low, high) in normal_ranges.items():\n        data[f'{test}_normalized'] = (data[test] - low) / (high - low)\n        col_drop.append(f'{test}_normalized')\n        \n\n    data['Liver_Function_Composite'] = data[['Bilirubin_normalized', 'Alk_Phos_normalized', 'SGOT_normalized', 'Albumin_normalized']].mean(axis=1)\n    data['Cholesterol_Tryglicerides_Ratio'] = data['Cholesterol'] / data['Tryglicerides']\n    data['Prothrombin_Platelets_Ratio'] = data['Prothrombin'] / data['Platelets']\n#     data['Edema_encoded'] = data['Edema'].map({'Y': 1, 'N': 0})\n#     data['Albumin_Edema_Interaction'] = data['Albumin'] * data['Edema_encoded']\n    data['Ascites_score'] = data['Ascites'].map({'Y': 1, 'N': 0})  # Assuming 'Y' for Yes and 'N' for No\n    data['Bilirubin_score'] = (data['Bilirubin'] > 1.2).astype(int)  # Elevated bilirubin\n    data['Albumin_score'] = (data['Albumin'] < 3.5).astype(int)  # Low albumin\n    data['Platelets_score'] = (data['Platelets'] < 150).astype(int)  # Low platelets\n    data['Alk_Phos_score'] = (data['Alk_Phos'] > 147).astype(int)  # Elevated Alk_Phos\n    data['SGOT_score'] = (data['SGOT'] > 40).astype(int)  # Elevated SGOT\n    data['Prothrombin_score'] = (data['Prothrombin'] > 12).astype(int)  # Prolonged prothrombin time\n    data['Spiders_score'] = data['Spiders'].map({'Y': 1, 'N': 0})\n    data['Cirrhosis_Composite_Score'] = data[['Ascites_score', 'Bilirubin_score', 'Albumin_score', 'Platelets_score', 'Alk_Phos_score', 'SGOT_score', 'Prothrombin_score', 'Spiders_score']].sum(axis=1)\n    col_drop.extend(['Liver_Function_Composite','Cholesterol_Tryglicerides_Ratio','Prothrombin_Platelets_Ratio', 'Drug', 'Bilirubin', 'Ascites_score', 'Albumin_score', 'Platelets_score', 'Alk_Phos_score', 'SGOT_score', 'Prothrombin_score', 'Spiders_score'])\n    data = data.drop(col_drop, axis = 1)    \n    return data\n\n# columns_wanted = ['Edema','Albumin','Cirrhosis_Composite_Score', 'Prothrombin_Platelets_Ratio', 'Age', 'Sex', 'N_Days', 'Drug', 'Hepatomegaly', 'Copper', 'Stage', 'Status']\n# columns_wanted_test = ['Edema','Albumin','Cirrhosis_Composite_Score', 'Prothrombin_Platelets_Ratio', 'Age', 'Sex', 'N_Days', 'Drug', 'Hepatomegaly', 'Copper', 'Stage']\ntrain_df = engineer_features(train)\ntest_df = engineer_features(test)\nfeature_names = [col for col in train_df.columns if col != 'Status']\n# train_df = train_df[columns_wanted]\n# test_df = test_df[columns_wanted_test]","metadata":{"execution":{"iopub.status.busy":"2024-01-01T20:06:53.445900Z","iopub.execute_input":"2024-01-01T20:06:53.446518Z","iopub.status.idle":"2024-01-01T20:06:53.509943Z","shell.execute_reply.started":"2024-01-01T20:06:53.446473Z","shell.execute_reply":"2024-01-01T20:06:53.508564Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"feature_names","metadata":{"execution":{"iopub.status.busy":"2024-01-01T20:06:56.209608Z","iopub.execute_input":"2024-01-01T20:06:56.210052Z","iopub.status.idle":"2024-01-01T20:06:56.219417Z","shell.execute_reply.started":"2024-01-01T20:06:56.210016Z","shell.execute_reply":"2024-01-01T20:06:56.217782Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.fillna(0)\ntest_df = test_df.fillna(0)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T20:07:12.094737Z","iopub.execute_input":"2024-01-01T20:07:12.095251Z","iopub.status.idle":"2024-01-01T20:07:12.114666Z","shell.execute_reply.started":"2024-01-01T20:07:12.095208Z","shell.execute_reply":"2024-01-01T20:07:12.113313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def  get_gmm_class_features(feat,n):\n    gmm=GaussianMixture(n_components=n,random_state=42)\n    gmm.fit(train_df[feat].fillna(train_df[feat].median()).values.reshape(-1,1))\n    train_df[f'{feat}_class']=gmm.predict(train_df[feat].fillna(train_df[feat].median()).values.reshape(-1,1))\n    test_df[f'{feat}_class']=gmm.predict(test_df[feat].fillna(test_df[feat].median()).values.reshape(-1,1))\n    \nget_gmm_class_features('Bilirubin',5)\nget_gmm_class_features('Albumin',5)\nget_gmm_class_features('SGOT',5)\nget_gmm_class_features('Platelets',4)\nget_gmm_class_features('Prothrombin',4)\nget_gmm_class_features('Stage',4)\nget_gmm_class_features('Cholesterol',4)\nget_gmm_class_features('Alk_Phos',5)\nget_gmm_class_features('Age',5)\nget_gmm_class_features('Tryglicerides',4)\nget_gmm_class_features('Copper',4)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T00:55:47.871274Z","iopub.execute_input":"2024-01-01T00:55:47.871686Z","iopub.status.idle":"2024-01-01T00:55:49.751799Z","shell.execute_reply.started":"2024-01-01T00:55:47.871646Z","shell.execute_reply":"2024-01-01T00:55:49.750481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-01-01T00:48:44.317970Z","iopub.execute_input":"2024-01-01T00:48:44.318393Z","iopub.status.idle":"2024-01-01T00:48:44.330260Z","shell.execute_reply.started":"2024-01-01T00:48:44.318361Z","shell.execute_reply":"2024-01-01T00:48:44.329055Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_columns = [col for col in train_df.columns if train_df[col].dtype in ['int64','float64']]\ncategorical_columns = [col for col in train_df.columns if train_df[col].dtype in ['object', 'bool', 'category'] and col != 'Status']\ncategorical_columns.remove('Edema')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T00:52:08.650534Z","iopub.execute_input":"2024-01-01T00:52:08.650985Z","iopub.status.idle":"2024-01-01T00:52:08.660672Z","shell.execute_reply.started":"2024-01-01T00:52:08.650948Z","shell.execute_reply":"2024-01-01T00:52:08.659402Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"ordinal_transformer = Pipeline(steps=[\n    ('ordinal_encoder', OrdinalEncoder())\n])\n\nonehot_transformer = Pipeline(steps=[\n    ('onehot_encoder', OneHotEncoder())\n])\n\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('ohe', onehot_transformer, ['Edema']),\n        ('ord', ordinal_transformer, categorical_columns),\n    ]\n)\ncat_cols = ['Edema_N', 'Edema_S', 'Edema_Y'] + categorical_columns\n\ntrain_df[cat_cols] = preprocessor.fit_transform(train_df)\ntest_df[cat_cols] = preprocessor.transform(test_df)\ntrain_df.drop(columns = ['Edema'], inplace = True)\ntest_df.drop(columns = ['Edema'], inplace = True)\n\nlbl = LabelEncoder()\ntrain_df['Status'] = lbl.fit_transform(train_df['Status'])","metadata":{"execution":{"iopub.status.busy":"2024-01-01T00:52:11.827087Z","iopub.execute_input":"2024-01-01T00:52:11.828055Z","iopub.status.idle":"2024-01-01T00:52:11.908431Z","shell.execute_reply.started":"2024-01-01T00:52:11.828012Z","shell.execute_reply":"2024-01-01T00:52:11.907577Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"numerical_transformer = Pipeline(steps=[\n    ('scaling', RobustScaler())\n])\n\npreprocessor = ColumnTransformer(\n    transformers = [\n        ('num', numerical_transformer, numerical_columns),\n        \n    ]\n)\n\ntrain_df[numerical_columns] = preprocessor.fit_transform(train_df)\ntest_df[numerical_columns] = preprocessor.transform(test_df)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T00:52:15.317968Z","iopub.execute_input":"2024-01-01T00:52:15.318378Z","iopub.status.idle":"2024-01-01T00:52:15.378891Z","shell.execute_reply.started":"2024-01-01T00:52:15.318344Z","shell.execute_reply":"2024-01-01T00:52:15.377881Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.dtypes","metadata":{"execution":{"iopub.status.busy":"2024-01-01T00:52:27.936584Z","iopub.execute_input":"2024-01-01T00:52:27.936999Z","iopub.status.idle":"2024-01-01T00:52:27.947966Z","shell.execute_reply.started":"2024-01-01T00:52:27.936966Z","shell.execute_reply":"2024-01-01T00:52:27.946631Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"**Exploratory Data Analysis**\n\nIn this section we will explore aspects of the data to learn how we should attack making the model. We are looking for features that are correlated to our labels as well as any features that will need to be cleaned for missing values. ","metadata":{}},{"cell_type":"code","source":"# Here we look at our data to see what is in our dataframe \ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T23:26:32.732578Z","iopub.execute_input":"2023-12-31T23:26:32.732986Z","iopub.status.idle":"2023-12-31T23:26:32.767271Z","shell.execute_reply.started":"2023-12-31T23:26:32.732953Z","shell.execute_reply":"2023-12-31T23:26:32.765935Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# get the basic statistics of the numerical columns of the training dataset\ntrain.describe()","metadata":{"execution":{"iopub.status.busy":"2023-12-28T23:51:21.423449Z","iopub.execute_input":"2023-12-28T23:51:21.423717Z","iopub.status.idle":"2023-12-28T23:51:21.470869Z","shell.execute_reply.started":"2023-12-28T23:51:21.423695Z","shell.execute_reply":"2023-12-28T23:51:21.469929Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Results of preliminary analysis: \n1. Some features will need to be scaled because they have difference ranges\n2. Some of the features have a large std meaning they likely won't make good features. I might consider dropping those features later. \n3. The histograms in the data section of this competition give a much better breakdown of the data than I did here. I would encourage you to go and check out the data here: https://www.kaggle.com/competitions/playground-series-s3e26/data?select=train.csv","metadata":{}},{"cell_type":"code","source":"# Preparing the data for a correlation matrix analysis by encoding the catagorical\n# into numerical features\n\n# Initialize LabelEncoder\nlabel_encoder = LabelEncoder()\n\n# Change categorical values to numerical values\nfor column in train_df.columns:\n    if train_df[column].dtype in ['object', 'bool', 'category']:  # Check if the column contains categorical data\n        train_df[column] = label_encoder.fit_transform(train_df[column])\n        if column in test.columns:\n            test_df[column] = label_encoder.transform(test_df[column])\n\n        \n# Show the dataframe\ntrain_df.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T20:07:27.528885Z","iopub.execute_input":"2024-01-01T20:07:27.529415Z","iopub.status.idle":"2024-01-01T20:07:27.589018Z","shell.execute_reply.started":"2024-01-01T20:07:27.529367Z","shell.execute_reply":"2024-01-01T20:07:27.587322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-28T23:51:21.515073Z","iopub.execute_input":"2023-12-28T23:51:21.515301Z","iopub.status.idle":"2023-12-28T23:51:21.532771Z","shell.execute_reply.started":"2023-12-28T23:51:21.515281Z","shell.execute_reply":"2023-12-28T23:51:21.531936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Create a correlation matrix using pandas\ncorr_matrix = train_df.corr()\n# use seaborn to make a heatmap of the corr_matrix\nsn.heatmap(corr_matrix)\n# show the plot using matplotlib\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:02.168512Z","iopub.execute_input":"2024-01-01T17:42:02.169774Z","iopub.status.idle":"2024-01-01T17:42:02.682271Z","shell.execute_reply.started":"2024-01-01T17:42:02.169715Z","shell.execute_reply":"2024-01-01T17:42:02.680870Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# I had a hard time differentiating between the colors, so I printed out\n# the correlations so I could better understand the data. The last row of the \n# correlation matrix has our correlations between our status and every other\n# feature.\nprint(corr_matrix.iloc[-1,:])","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:42:14.214373Z","iopub.execute_input":"2024-01-01T17:42:14.214786Z","iopub.status.idle":"2024-01-01T17:42:14.222758Z","shell.execute_reply.started":"2024-01-01T17:42:14.214754Z","shell.execute_reply":"2024-01-01T17:42:14.221415Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Based on this analysis the following features are canidates for using in our model: Ascites, Hepatomegaly, Spiders, Edema, Bilirubin, Copper, Albumin, SGOT, Prothrombin, and Stage \n\nNext I need to invetigate what type of relationship these variables have with the target. ","metadata":{}},{"cell_type":"code","source":"x = train_df['Status']\ny = train_df['N_Days']\nslope, intercept = np.polyfit(x, y, 1)\nplt.scatter(x, y)\nplt.plot(x, slope * x + intercept, color='red', label='Line of Best Fit')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2023-12-28T23:51:21.949606Z","iopub.execute_input":"2023-12-28T23:51:21.950196Z","iopub.status.idle":"2023-12-28T23:51:22.151719Z","shell.execute_reply.started":"2023-12-28T23:51:21.950167Z","shell.execute_reply":"2023-12-28T23:51:22.150838Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Types of relationships: \n\nStatus and Hepatomegaly: non linear","metadata":{}},{"cell_type":"code","source":"y = train_df['Status']\nX = train_df.drop(['Status'], axis=1)\nscaler = RobustScaler()\nX = scaler.fit_transform(X)\n# pca = PCA(n_components=2)  # Adjust n_components as needed\n# X = pca.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T20:07:34.552075Z","iopub.execute_input":"2024-01-01T20:07:34.552514Z","iopub.status.idle":"2024-01-01T20:07:34.582555Z","shell.execute_reply.started":"2024-01-01T20:07:34.552479Z","shell.execute_reply":"2024-01-01T20:07:34.581059Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train.shape","metadata":{"execution":{"iopub.status.busy":"2024-01-01T17:44:22.737151Z","iopub.execute_input":"2024-01-01T17:44:22.738574Z","iopub.status.idle":"2024-01-01T17:44:22.746662Z","shell.execute_reply.started":"2024-01-01T17:44:22.738516Z","shell.execute_reply":"2024-01-01T17:44:22.745412Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Simple linear model build and tested with our training set","metadata":{}},{"cell_type":"code","source":"# test_df = test_df.drop('id', axis=1)\nX_test_real = scaler.transform(test_df)\n# X_test_real = test_df.drop('id', axis=1)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T15:12:43.395468Z","iopub.execute_input":"2024-01-01T15:12:43.396372Z","iopub.status.idle":"2024-01-01T15:12:43.402101Z","shell.execute_reply.started":"2024-01-01T15:12:43.396342Z","shell.execute_reply":"2024-01-01T15:12:43.401330Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-12-31T14:57:27.978516Z","iopub.execute_input":"2023-12-31T14:57:27.978925Z","iopub.status.idle":"2023-12-31T14:57:28.222492Z","shell.execute_reply.started":"2023-12-31T14:57:27.978865Z","shell.execute_reply":"2023-12-31T14:57:28.221080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{"execution":{"iopub.status.busy":"2023-12-31T14:57:30.927166Z","iopub.execute_input":"2023-12-31T14:57:30.927630Z","iopub.status.idle":"2023-12-31T14:57:30.941070Z","shell.execute_reply.started":"2023-12-31T14:57:30.927589Z","shell.execute_reply":"2023-12-31T14:57:30.938988Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.isna().sum()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T14:33:40.117392Z","iopub.execute_input":"2024-01-01T14:33:40.117738Z","iopub.status.idle":"2024-01-01T14:33:40.127405Z","shell.execute_reply.started":"2024-01-01T14:33:40.117712Z","shell.execute_reply":"2024-01-01T14:33:40.126317Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = 'submission.csv'\ndf.to_csv(file_path) ","metadata":{"execution":{"iopub.status.busy":"2023-12-31T14:57:39.373377Z","iopub.execute_input":"2023-12-31T14:57:39.373769Z","iopub.status.idle":"2023-12-31T14:57:39.430369Z","shell.execute_reply.started":"2023-12-31T14:57:39.373740Z","shell.execute_reply":"2023-12-31T14:57:39.429257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Here we create a simple random forest ensemble","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    # Define hyperparameters to optimize\n    n_estimators = trial.suggest_int(\"n_estimators\", 50, 200)\n#     max_depth = trial.suggest_int(\"max_depth\", 2, 32, log=True)\n#     min_samples_split = trial.suggest_float(\"min_samples_split\", 0.1, 1.0)\n    min_samples_leaf = trial.suggest_int(\"min_samples_leaf\", 1, 50)\n\n    # Initialize the Random Forest classifier with hyperparameters\n    clf = RandomForestClassifier(\n        n_estimators=n_estimators,\n#         max_depth=max_depth,\n#         min_samples_split=min_samples_split,\n        min_samples_leaf=min_samples_leaf,\n        random_state=42\n    )\n\n    # Train the classifier\n    clf.fit(X_train, y_train)\n\n    # Predict probabilities on the validation set\n    y_prob = clf.predict_proba(X_test)\n\n    # Calculate log loss as the metric to optimize\n    logloss = log_loss(y_test, y_prob)\n\n    return logloss\n\nstudy = optuna.create_study(direction=\"minimize\")  # or \"minimize\" for a different metric\nstudy.optimize(objective, n_trials=50)  # You can adjust the number of trials\n\n# Print the best hyperparameters and score\nbest_params = study.best_params\nbest_score = study.best_value\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best Score:\", best_score)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T20:44:39.016413Z","iopub.execute_input":"2024-01-01T20:44:39.016901Z","iopub.status.idle":"2024-01-01T20:46:00.130476Z","shell.execute_reply.started":"2024-01-01T20:44:39.016865Z","shell.execute_reply":"2024-01-01T20:46:00.128954Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_forest_model = RandomForestClassifier(n_estimators=154, min_samples_leaf=3, random_state=42)\nrandom_forest_model.fit(X_train, y_train)\ny_pred_forest_val = random_forest_model.predict_proba(X_test)\naccuracy = log_loss(y_test, y_pred_forest_val)\nprint(f'Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T20:46:13.695095Z","iopub.execute_input":"2024-01-01T20:46:13.696132Z","iopub.status.idle":"2024-01-01T20:46:15.973007Z","shell.execute_reply.started":"2024-01-01T20:46:13.696087Z","shell.execute_reply":"2024-01-01T20:46:15.971975Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred_forest = random_forest_model.predict_proba(X_test_real)\ndf = pd.DataFrame(y_pred_forest, columns=['Status_C', 'Status_CL', 'Status_D'], index=index)\ndf.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T23:01:08.122864Z","iopub.execute_input":"2023-12-31T23:01:08.123325Z","iopub.status.idle":"2023-12-31T23:01:08.229028Z","shell.execute_reply.started":"2023-12-31T23:01:08.123289Z","shell.execute_reply":"2023-12-31T23:01:08.228238Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = 'submission_forest.csv'\ndf.to_csv(file_path) ","metadata":{"execution":{"iopub.status.busy":"2023-12-31T19:38:39.359023Z","iopub.execute_input":"2023-12-31T19:38:39.359397Z","iopub.status.idle":"2023-12-31T19:38:39.391733Z","shell.execute_reply.started":"2023-12-31T19:38:39.359368Z","shell.execute_reply":"2023-12-31T19:38:39.390528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To do: \n1. Create loss function to test models before submission.... CHECK\n2. Try out a neural network to see if it will outperform a tree classifier\n    a. My guess is that it should\n3. I should also scale the data and rerun the log regression. \n4. Develop a cross validation strategy\n5. develop function that will optomize learning parameters","metadata":{}},{"cell_type":"code","source":"num_classes = 3 \n\nparams = {\n    'objective': 'multi:softprob',  # Multi-class classification\n    'eval_metric': 'mlogloss',     # Evaluation metric (log loss)\n    'num_class': num_classes,      # Number of classes\n    'max_depth': 4,                # Maximum depth of trees\n    'learning_rate': 0.1,\n    'n_estimators': 100\n}\n\n# Create a DMatrix from your training data\ndtrain = xgb.DMatrix(X_train, label=y_train)\n\n# Train the XGBoost model\nmodel = xgb.train(params, dtrain, num_boost_round=100)\n\ndtest = xgb.DMatrix(X_test)\n\n# Make predictions\npredictions_tree = model.predict(dtest)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T18:29:32.588732Z","iopub.execute_input":"2024-01-01T18:29:32.589180Z","iopub.status.idle":"2024-01-01T18:29:33.031977Z","shell.execute_reply.started":"2024-01-01T18:29:32.589145Z","shell.execute_reply":"2024-01-01T18:29:33.031026Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(predictions_tree)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T14:58:42.671490Z","iopub.execute_input":"2023-12-31T14:58:42.671965Z","iopub.status.idle":"2023-12-31T14:58:42.679336Z","shell.execute_reply.started":"2023-12-31T14:58:42.671932Z","shell.execute_reply":"2023-12-31T14:58:42.678149Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = log_loss(y_test, predictions_tree)\nprint(f'Log Loss: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2023-12-31T19:38:48.318664Z","iopub.execute_input":"2023-12-31T19:38:48.319038Z","iopub.status.idle":"2023-12-31T19:38:48.329113Z","shell.execute_reply.started":"2023-12-31T19:38:48.319008Z","shell.execute_reply":"2023-12-31T19:38:48.327967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"xtest = xgb.DMatrix(X_test_real)\npredictions_test = model.predict(xtest)\ndf_xboost = pd.DataFrame(predictions_test, columns=['Status_C', 'Status_CL', 'Status_D'], index=test['id'])\ndf_xboost.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-31T19:38:53.065498Z","iopub.execute_input":"2023-12-31T19:38:53.065867Z","iopub.status.idle":"2023-12-31T19:38:53.091694Z","shell.execute_reply.started":"2023-12-31T19:38:53.065839Z","shell.execute_reply":"2023-12-31T19:38:53.090157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\n\ndef objective(trial):\n    params = {\n        \"objective\": \"multi_logloss\",\n        \"n_estimators\": trial.suggest_int('n_estimators', 500, 750),\n        \"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-3, 0.1, log=True),\n        \"max_depth\": trial.suggest_int(\"max_depth\", 1, 10),\n        \"subsample\": trial.suggest_float(\"subsample\", 0.05, 1.0), \n        \"colsample_bytree\": trial.suggest_float(\"colsample_bytree\", 0.05, 1.0), \n        \"min_child_weight\": trial.suggest_int(\"min_child_weight\", 1, 20),\n    }\n\n    # Build the xgboost model\n    optuna_xgbmodel = xgb.XGBClassifier(**params,\n                                    random_state=42)\n    \n    optuna_xgbmodel.fit(X_train, y_train)\n    y_pred_probs = optuna_xgbmodel.predict_proba(X_test)\n    logloss = log_loss(y_test, y_pred_probs)\n    return logloss\n\nstudy = optuna.create_study(direction='minimize')\nstudy.optimize(objective, n_trials=50)\n\nprint(study.best_params)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T20:08:32.409455Z","iopub.execute_input":"2024-01-01T20:08:32.409934Z","iopub.status.idle":"2024-01-01T20:11:17.732441Z","shell.execute_reply.started":"2024-01-01T20:08:32.409898Z","shell.execute_reply":"2024-01-01T20:11:17.731201Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"{'n_estimators': 666, 'learning_rate': 0.09792447281018261, 'max_depth': 3, 'subsample': 0.8802526465250708, 'colsample_bytree': 0.12328759472404174, 'min_child_weight': 9} with feature engineering\n\nold params \n'objective': 'multi:softprob',  # Multi-class classification\n    'eval_metric': 'mlogloss',     # Evaluation metric (log loss)\n    'num_class': num_classes,      # Number of classes\n    'max_depth': 10,                # Maximum depth of trees\n    'learning_rate': 0.03970804555039867,\n    'n_estimators': 572,\n    'subsample': 0.5048914610683402,\n    'colsample_bytree': 0.1299678713013594,\n    'min_child_weight': 14,\n    'enable_categorical': True","metadata":{}},{"cell_type":"code","source":"num_classes = 3 \n\nparams = {\n    'objective': 'multi:softprob',  # Multi-class classification\n    'eval_metric': 'mlogloss',     # Evaluation metric (log loss)\n    'num_class': num_classes,      # Number of classes\n    'max_depth': 8,                # Maximum depth of trees\n    'learning_rate': 0.03733937019630027,\n    'n_estimators': 707,\n    'subsample': 0.7713822447289903,\n    'colsample_bytree': 0.11897302935991551,\n    'min_child_weight': 14,\n    \n}\n\n\n# Train the XGBoost model\nmodel_2 = xgb.XGBClassifier(**params, random_state=42)\nmodel_2.fit(X_train, y_train)\n\n# Make predictions\npredictions_xgb_test = model_2.predict_proba(X_test)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T20:48:07.670518Z","iopub.execute_input":"2024-01-01T20:48:07.671110Z","iopub.status.idle":"2024-01-01T20:48:11.004229Z","shell.execute_reply.started":"2024-01-01T20:48:07.671072Z","shell.execute_reply":"2024-01-01T20:48:11.003212Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"params = {\n    'objective': 'multi:softprob',  # Multi-class classification\n    'eval_metric': 'mlogloss',     # Evaluation metric (log loss)\n    'num_class': num_classes,      # Number of classes\n    'max_depth': 3,                # Maximum depth of trees\n    'learning_rate': 0.09792447281018261,\n    'n_estimators': 572,\n    'subsample': 0.8802526465250708,\n    'colsample_bytree': 0.12328759472404174,\n    'min_child_weight': 9,","metadata":{}},{"cell_type":"code","source":"importances = model_2.feature_importances_\n\n\n# Plotting feature importances\nplt.figure(figsize=(10, 6))\nplt.bar(range(len(importances)), importances)\nplt.xticks(range(len(importances)), feature_names, rotation=90)\nplt.title('Feature Importances')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T20:48:23.215660Z","iopub.execute_input":"2024-01-01T20:48:23.216446Z","iopub.status.idle":"2024-01-01T20:48:23.639639Z","shell.execute_reply.started":"2024-01-01T20:48:23.216407Z","shell.execute_reply":"2024-01-01T20:48:23.638199Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions_test = model_2.predict_proba(X_test_real)\ndf_xboost = pd.DataFrame(predictions_test, columns=['Status_C', 'Status_CL', 'Status_D'], index=index)\ndf_xboost.head()","metadata":{"execution":{"iopub.status.busy":"2024-01-01T19:33:29.584552Z","iopub.execute_input":"2024-01-01T19:33:29.584966Z","iopub.status.idle":"2024-01-01T19:33:29.628012Z","shell.execute_reply.started":"2024-01-01T19:33:29.584934Z","shell.execute_reply":"2024-01-01T19:33:29.626247Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"accuracy = log_loss(y_test, predictions_xgb_test)\nprint(f'Accuracy: {accuracy}')","metadata":{"execution":{"iopub.status.busy":"2024-01-01T20:48:18.971174Z","iopub.execute_input":"2024-01-01T20:48:18.971584Z","iopub.status.idle":"2024-01-01T20:48:18.982100Z","shell.execute_reply.started":"2024-01-01T20:48:18.971552Z","shell.execute_reply":"2024-01-01T20:48:18.980898Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = 'submission_xboost.csv'\ndf_xboost.to_csv(file_path) ","metadata":{"execution":{"iopub.status.busy":"2023-12-31T19:40:57.693304Z","iopub.execute_input":"2023-12-31T19:40:57.693682Z","iopub.status.idle":"2023-12-31T19:40:57.715492Z","shell.execute_reply.started":"2023-12-31T19:40:57.693654Z","shell.execute_reply":"2023-12-31T19:40:57.714076Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"{'n_estimators': 572, 'learning_rate': 0.03970804555039867, 'max_depth': 10, 'subsample': 0.5048914610683402, 'colsample_bytree': 0.1299678713013594, 'min_child_weight': 14}","metadata":{}},{"cell_type":"code","source":"print(X_train.shape, y_train.shape)","metadata":{"execution":{"iopub.status.busy":"2023-12-30T17:14:48.097180Z","iopub.execute_input":"2023-12-30T17:14:48.097663Z","iopub.status.idle":"2023-12-30T17:14:48.104233Z","shell.execute_reply.started":"2023-12-30T17:14:48.097624Z","shell.execute_reply":"2023-12-30T17:14:48.102707Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"To do: \n1. create neural network and tune it\n2. stack the neural network with xgboost with logistic regression as the meta-learner\n    a. try it with orginal features \n    b. try it without orginal features\n3. scale the data","metadata":{}},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow import keras\nfrom keras.utils import to_categorical\nfrom tensorflow.keras.callbacks import EarlyStopping\nfrom tensorflow.keras.layers import Dense, Dropout\n\n\n# Define the neural network model\nnn_model = keras.Sequential([\n    keras.layers.Dense(204, activation='relu', input_shape=(X_train.shape[1],)),\n    Dropout(0.367790476008353),\n    keras.layers.Dense(3, activation='softmax')  # Three classes, softmax activation for multi-class\n])\n\nearly_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n# Compile the model with log loss (cross-entropy)\nnn_model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.0011553897364248928), loss='categorical_crossentropy', metrics=['accuracy'])\n\ny_train_onehot = to_categorical(y_train, num_classes=3)\ny_test_onehot = to_categorical(y_test, num_classes=3)\n# Train the model\nnn_model.fit(X_train, y_train_onehot, epochs=50, batch_size=32, validation_split=0.2, callbacks=[early_stopping])  # Adjust epochs and batch size\n\n\n# Evaluate the model on test data with log loss\ntest_loss, test_accuracy = nn_model.evaluate(X_test, y_test_onehot)\nprint(f\"Test Log Loss: {test_loss:.4f}\")\n\npredictions_nn_test = nn_model.predict(X_test)\n\npredictions_nn = nn_model.predict(X_test_real)\nprint(predictions_nn)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T21:27:06.163019Z","iopub.execute_input":"2023-12-31T21:27:06.163478Z","iopub.status.idle":"2023-12-31T21:27:16.318836Z","shell.execute_reply.started":"2023-12-31T21:27:06.163440Z","shell.execute_reply":"2023-12-31T21:27:16.316839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport tensorflow as tf\nfrom tensorflow import keras\nfrom keras.utils import to_categorical\nfrom tensorflow.keras.layers import Dense, Dropout\nfrom tensorflow.keras.callbacks import EarlyStopping\nimport numpy as np\n\n# Define the objective function to optimize\ndef objective(trial):\n    # Define hyperparameters to search\n    learning_rate = trial.suggest_float('learning_rate', 1e-5, 1e-1, log=True)\n    num_units = trial.suggest_int('num_units', 16, 256, log=True)\n    dropout_rate = trial.suggest_float('dropout_rate', 0.0, 0.5)\n#     num_hidden_layers = trial.suggest_int('num_hidden_layers', 1, 5)\n    \n    model = keras.Sequential()\n    # Build and compile the neural network with the sampled hyperparameters\n    model.add(Dense(num_units, activation='relu', input_shape=(X_train.shape[1],)))\n    model.add(Dropout(dropout_rate)) \n\n#     for _ in range(num_hidden_layers):\n#         model.add(Dense(num_units, activation='relu'))\n#         model.add(Dropout(dropout_rate))  # Dropout layer for regularization\n\n    model.add(Dense(3, activation='softmax'))\n    \n    model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=learning_rate),\n                  loss='categorical_crossentropy',\n                  metrics=['accuracy'])\n\n    # Early stopping callback\n    early_stopping = EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True)\n    y_train_onehot = to_categorical(y_train, num_classes=3)\n    y_test_onehot = to_categorical(y_test, num_classes=3)\n\n    # Train the model\n    history = model.fit(X_train, y_train_onehot, epochs=50, batch_size=32, validation_split=0.2, verbose=0, callbacks=[early_stopping])\n\n    # Evaluate and return the validation loss (to minimize)\n    val_loss = history.history['val_loss'][-1]\n    return val_loss\n\n# Create an Optuna study for hyperparameter optimization\nstudy = optuna.create_study(direction='minimize')\n\n# Run the optimization for a specified number of trials\nstudy.optimize(objective, n_trials=30)\n\n# Get the best hyperparameters and results\nbest_params = study.best_params\nbest_log_loss = study.best_value\n\nprint(f\"Best Hyperparameters: {best_params}\")\nprint(f\"Best Log Loss: {best_log_loss:.4f}\")\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T01:07:50.997994Z","iopub.execute_input":"2024-01-01T01:07:50.998409Z","iopub.status.idle":"2024-01-01T01:13:58.523169Z","shell.execute_reply.started":"2024-01-01T01:07:50.998376Z","shell.execute_reply":"2024-01-01T01:13:58.521980Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"1 layer\nBest Hyperparameters: {'learning_rate': 0.013170397942564903, 'num_units': 185}\nBest Negative Log Loss: 0.6375\n\nBest Hyperparameters: {'learning_rate': 0.0011553897364248928, 'num_units': 204, 'dropout_rate': 0.367790476008353}\nBest Log Loss: 0.6235\n\nBest Hyperparameters: {'learning_rate': 0.0009783562667604346, 'num_units': 31, 'dropout_rate': 0.06061700034277989, 'num_hidden_layers': 4}\nBest Log Loss: 0.6521\n\nscalled mult layers: Best Hyperparameters: {'learning_rate': 0.0005053505112715387, 'num_units': 42, 'dropout_rate': 0.36510169582829427, 'num_hidden_layers': 1}\nBest Log Loss: 0.5100\n\nno hidden layers\nBest Hyperparameters: {'learning_rate': 0.0029784197548427148, 'num_units': 31, 'dropout_rate': 0.17430692789044092}\nBest Log Loss: 0.4999","metadata":{}},{"cell_type":"code","source":"df_nn = pd.DataFrame(predictions_nn, columns=['Status_C', 'Status_CL', 'Status_D'], index=index)\ndf_nn.head()","metadata":{"execution":{"iopub.status.busy":"2023-12-30T17:24:56.702377Z","iopub.execute_input":"2023-12-30T17:24:56.702817Z","iopub.status.idle":"2023-12-30T17:24:56.718174Z","shell.execute_reply.started":"2023-12-30T17:24:56.702782Z","shell.execute_reply":"2023-12-30T17:24:56.717020Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"file_path = 'submission_nn.csv'\ndf_nn.to_csv(file_path) ","metadata":{"execution":{"iopub.status.busy":"2023-12-30T17:25:35.955292Z","iopub.execute_input":"2023-12-30T17:25:35.955729Z","iopub.status.idle":"2023-12-30T17:25:36.002163Z","shell.execute_reply.started":"2023-12-30T17:25:35.955693Z","shell.execute_reply":"2023-12-30T17:25:36.001169Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.metrics import log_loss\n\n# Define the objective function for KNeighborsClassifier\ndef knn_objective(trial):\n    n_neighbors = trial.suggest_int('n_neighbors', 1, 20)\n    \n    knn_classifier = KNeighborsClassifier(n_neighbors=n_neighbors)\n    knn_classifier.fit(X_train, y_train)\n    knn_probs = knn_classifier.predict_proba(X_test)\n    \n    logloss = log_loss(y_test, knn_probs)\n    return logloss\n\n# Define the objective function for SVC\ndef svc_objective(trial):\n    C = trial.suggest_float('C', 0.1, 10.0)\n    kernel = trial.suggest_categorical('kernel', ['linear', 'poly', 'rbf', 'sigmoid'])\n    \n    svc_classifier = SVC(C=C, kernel=kernel, probability=True)\n    svc_classifier.fit(X_train, y_train)\n    svc_probs = svc_classifier.predict_proba(X_test)\n    \n    logloss = log_loss(y_test, svc_probs)\n    return logloss\n\n\n# Create an Optuna study for each model's hyperparameter optimization\nknn_study = optuna.create_study(direction='minimize')\nsvc_study = optuna.create_study(direction='minimize')\n\n# Optimize hyperparameters for KNeighborsClassifier\nknn_study.optimize(knn_objective, n_trials=50)\n\n# Optimize hyperparameters for SVC\nsvc_study.optimize(svc_objective, n_trials=50)\n\n# Get the best hyperparameters and results for each model\nbest_knn_params = knn_study.best_params\nbest_svc_params = svc_study.best_params\nknn_logloss = knn_study.best_value\nsvc_logloss = svc_study.best_value\n\n\nprint(\"Best Hyperparameters for KNeighborsClassifier:\", best_knn_params)\nprint(\"Best Hyperparameters for SVC:\", best_svc_params)\nprint(\"KNeighborsClassifier Log Loss:\", knn_logloss)\nprint(\"SVC Log Loss:\", svc_logloss)\n\n","metadata":{"execution":{"iopub.status.busy":"2023-12-31T22:51:45.540696Z","iopub.execute_input":"2023-12-31T22:51:45.541139Z","iopub.status.idle":"2023-12-31T22:51:55.955866Z","shell.execute_reply.started":"2023-12-31T22:51:45.541103Z","shell.execute_reply":"2023-12-31T22:51:55.954289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import BaggingClassifier\n\n# Assuming you have already split your data into X_train and y_train for training\n# and X_test for testing\n\n# Create and train the KNeighborsClassifier\nknn_classifier = KNeighborsClassifier(n_neighbors=20) # You can adjust the number of neighbors\nbagged_knn = BaggingClassifier(knn_classifier, n_estimators=100, random_state=42)\nbagged_knn.fit(X_train, y_train)\n\n# Make predictions using KNeighborsClassifier\nknn_preds = bagged_knn.predict_proba(X_test)\n# knn_preds_real = bagged_knn.predict_proba(X_test_real)\n\n# Calculate accuracy for KNeighborsClassifier\nknn_accuracy = log_loss(y_test, knn_preds)\nprint(\"KNeighborsClassifier Accuracy:\", knn_accuracy)\n\n# Create and train the Support Vector Classifier (SVC)\n# svc_classifier = SVC(kernel='rbf', C=2.367159073790816)# You can choose a different kernel and C value\n# bagged_svc = BaggingClassifier(svc_classifier, n_estimators=100, random_state=42)\n# bagged_svc.fit(X_train, y_train)\n\n# # Make predictions using SVC\n# svc_preds = bagged_svc.decision_function(X_test)\n# svc_preds_real = bagged_svc.decision_function(X_test_real)\n\n# # Calculate accuracy for SVC\n# svc_accuracy = log_loss(y_test, svc_preds)\n# print(\"SVC Accuracy:\", svc_accuracy)\n\n# # Create and train the Gaussian Naive Bayes (GaussianNB) classifier\n# gnb_classifier = GaussianNB()\n# bagged_gnb = BaggingClassifier(gnb_classifier, n_estimators=100, random_state=42)\n# bagged_gnb.fit(X_train, y_train)\n\n# # Make predictions using GaussianNB\n# gnb_preds = bagged_gnb.predict_proba(X_test)\n# gnb_preds_real = bagged_gnb.predict_proba(X_test_real)\n\n# # Calculate accuracy for GaussianNB\n# gnb_accuracy = log_loss(y_test, gnb_preds)\n# print(\"GaussianNB Accuracy:\", gnb_accuracy)\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T19:53:53.168241Z","iopub.execute_input":"2024-01-01T19:53:53.168886Z","iopub.status.idle":"2024-01-01T19:53:57.866909Z","shell.execute_reply.started":"2024-01-01T19:53:53.168810Z","shell.execute_reply":"2024-01-01T19:53:57.865901Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import optuna\nimport lightgbm as lgb\nimport numpy as np\nimport pandas as pd\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import log_loss\n\n# Define the LightGBM objective function for Optuna\ndef objective(trial):\n    # Define hyperparameters to optimize\n    params = {\n        \"objective\": \"multiclass\",\n        \"metric\": \"multi_logloss\",\n        \"num_iterations\": trial.suggest_int(\"num_iterations\", 50, 200 ),\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"num_class\": 3,\n        \"lambda_l1\": trial.suggest_float(\"lambda_l1\", 1e-8, 10.0, log=True),\n        \"lambda_l2\": trial.suggest_float(\"lambda_l2\", 1e-8, 10.0, log=True),\n        \"num_leaves\": trial.suggest_int(\"num_leaves\", 2, 456),\n        \"feature_fraction\": trial.suggest_float(\"feature_fraction\", 0.2, 1.0),\n        \"bagging_fraction\": trial.suggest_float(\"bagging_fraction\", 0.2, 1.0),\n        \"bagging_freq\": trial.suggest_int(\"bagging_freq\", 1, 7),\n        \"min_child_samples\": trial.suggest_int(\"min_child_samples\", 2, 100),\n        \"learning_rate\": trial.suggest_float('learning_rate', 9e-2, .12, log=True)\n    }\n\n    # Create and train a LightGBM classifier\n    clf = lgb.LGBMClassifier(**params, random_state=42)\n    clf.fit(X_train, y_train)\n\n    # Predict probabilities on the validation set\n    y_prob = clf.predict_proba(X_test)\n\n    # Calculate log loss as the metric to optimize\n    logloss = log_loss(y_test, y_prob)\n\n    return logloss\n\n# Create an Optuna study\nstudy = optuna.create_study(direction='minimize')  # We want to minimize log loss\n\n# Run optimization (you can change the number of trials as needed)\nstudy.optimize(objective, n_trials=50)\n\n# Print the best hyperparameters and score\nbest_params = study.best_params\nbest_score = study.best_value\nprint(\"Best Hyperparameters:\", best_params)\nprint(\"Best Log Loss:\", best_score)\n\n# Train the final model using the best hyperparameters\nbest_params['objective'] = 'binary'\nbest_params['metric'] = 'binary_logloss'\n\n","metadata":{"execution":{"iopub.status.busy":"2024-01-01T20:57:25.796066Z","iopub.execute_input":"2024-01-01T20:57:25.796614Z","iopub.status.idle":"2024-01-01T21:01:22.725860Z","shell.execute_reply.started":"2024-01-01T20:57:25.796573Z","shell.execute_reply":"2024-01-01T21:01:22.724888Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"final_params = {\n        \"objective\": \"multiclass\",\n        \"metric\": \"multi_logloss\",\n        \"verbosity\": -1,\n        \"boosting_type\": \"gbdt\",\n        \"num_class\": 3,\n        'num_iterations': 90, \n        'lambda_l1': 5.277563111764755e-08, \n        'lambda_l2': 4.782766023009482e-06, \n        'num_leaves': 346, \n        'feature_fraction': 0.26790241031301165, \n        'bagging_fraction': 0.8183489454210847, \n        'bagging_freq': 2, \n        'min_child_samples': 67, \n        'learning_rate': 0.0922843047704934\n    }\n\n\n\n\nfinal_model = lgb.LGBMClassifier(**final_params, random_state=42)\nfinal_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T21:05:27.880096Z","iopub.execute_input":"2024-01-01T21:05:27.880697Z","iopub.status.idle":"2024-01-01T21:05:32.529312Z","shell.execute_reply.started":"2024-01-01T21:05:27.880657Z","shell.execute_reply":"2024-01-01T21:05:32.527698Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# stacked_preds = np.hstack((X_test, y_pred_forest_val, predictions_xgb_test, predictions_nn_test))\n# stacked_preds_test = np.hstack((X_test_real, y_pred_forest, predictions_test, predictions_nn))\n\n# stacked_preds = np.hstack((X_test, y_pred_forest_val, predictions_xgb_test, predictions_nn_test, gnb_preds, svc_preds, knn_preds))\n# stacked_preds_test = np.hstack((X_test_real, y_pred_forest, predictions_test, predictions_nn, gnb_preds_real, svc_preds_real, knn_preds_real))\n# print(stacked_preds)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T21:29:55.517286Z","iopub.execute_input":"2023-12-31T21:29:55.517641Z","iopub.status.idle":"2023-12-31T21:29:55.527703Z","shell.execute_reply.started":"2023-12-31T21:29:55.517609Z","shell.execute_reply":"2023-12-31T21:29:55.525861Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = LogisticRegression(multi_class='multinomial', solver='lbfgs', max_iter=10000, random_state=42)\nmodel.fit(stacked_preds, y_test)\ny_pred = model.predict_proba(stacked_preds)\naccuracy = log_loss(y_test, y_pred)\nprint(f'Accuracy: {accuracy}')\n# # Here we need probabilities not classifications. \ny_pred_real = model.predict_proba(stacked_preds_test)","metadata":{"execution":{"iopub.status.busy":"2023-12-31T21:29:58.893979Z","iopub.execute_input":"2023-12-31T21:29:58.894415Z","iopub.status.idle":"2023-12-31T21:29:59.000972Z","shell.execute_reply.started":"2023-12-31T21:29:58.894380Z","shell.execute_reply":"2023-12-31T21:29:58.999797Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.ensemble import StackingClassifier\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.model_selection import train_test_split, StratifiedKFold\n\n# Initializing Stratified K-Fold with 5 folds\nskf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n\n# Initializing an MLPClassifier\nmlp = MLPClassifier(\n    hidden_layer_sizes=(64, 32),\n    max_iter=1000,\n    random_state=42,\n    activation='relu',\n    learning_rate_init=0.001,\n    solver='adam',\n    validation_fraction=0.1,\n    momentum=0.9,\n    nesterovs_momentum=True,\n    batch_size=32,\n    beta_1=0.9,\n    beta_2=0.999\n)\n\n# Creating a StackingClassifier\nstacking_model = StackingClassifier(\n    estimators=[\n        ('XGB', model_2),\n#         ('RFM', random_forest_model),\n        ('LGBM', final_model)\n    ],\n    final_estimator=mlp,\n    cv=skf\n)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T21:08:15.993111Z","iopub.execute_input":"2024-01-01T21:08:15.993645Z","iopub.status.idle":"2024-01-01T21:08:16.004610Z","shell.execute_reply.started":"2024-01-01T21:08:15.993605Z","shell.execute_reply":"2024-01-01T21:08:16.002967Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"stacking_model.fit(X_train, y_train)","metadata":{"execution":{"iopub.status.busy":"2024-01-01T21:08:20.558116Z","iopub.execute_input":"2024-01-01T21:08:20.559053Z","iopub.status.idle":"2024-01-01T21:09:26.915012Z","shell.execute_reply.started":"2024-01-01T21:08:20.558997Z","shell.execute_reply":"2024-01-01T21:09:26.913805Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = stacking_model.predict_proba(X_test)\nlloss = log_loss(y_test, y_pred)\nprint(f\"Log loss on test data: {lloss}\")","metadata":{"execution":{"iopub.status.busy":"2024-01-01T21:09:30.792872Z","iopub.execute_input":"2024-01-01T21:09:30.793410Z","iopub.status.idle":"2024-01-01T21:09:31.015119Z","shell.execute_reply.started":"2024-01-01T21:09:30.793342Z","shell.execute_reply":"2024-01-01T21:09:31.012650Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df = pd.DataFrame(y_pred_real, columns=['Status_C', 'Status_CL', 'Status_D'], index=index)\ndf.head()\nfile_path = 'submission_stacked.csv'\ndf.to_csv(file_path) ","metadata":{"execution":{"iopub.status.busy":"2023-12-31T21:28:12.810101Z","iopub.execute_input":"2023-12-31T21:28:12.810500Z","iopub.status.idle":"2023-12-31T21:28:12.840485Z","shell.execute_reply.started":"2023-12-31T21:28:12.810469Z","shell.execute_reply":"2023-12-31T21:28:12.839581Z"},"trusted":true},"execution_count":null,"outputs":[]}]}